{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18e8b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è sentence-transformers not installed. Using TF-IDF embeddings instead.\n",
      "‚úÖ Setup complete\n",
      "Root Directory: C:\\Users\\aishw\\seo-content-detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports & Directory Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import textstat\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Optional: sentence transformers for better embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTE = True\n",
    "except ImportError:\n",
    "    SENTE = False\n",
    "    print(\"‚ö†Ô∏è sentence-transformers not installed. Using TF-IDF embeddings instead.\")\n",
    "\n",
    "# Download NLTK tokenizer \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Directory Structure\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "\n",
    "# File paths\n",
    "INPUT_CSV = DATA_DIR / \"data.csv\"\n",
    "EXTRACTED_CSV = DATA_DIR / \"extracted_content.csv\"\n",
    "FEATURES_CSV = DATA_DIR / \"features.csv\"\n",
    "DUPLICATES_CSV = DATA_DIR / \"duplicates.csv\"\n",
    "MODEL_PATH = MODELS_DIR / \"quality_model.pkl\"\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(\"Root Directory:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa735a53",
   "metadata": {},
   "source": [
    "### HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "557a3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Parsing Functions\n",
    "def extract_from_html(html: str):\n",
    "    \"\"\"Extracts title, main body text, and word count from raw HTML.\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        title = soup.title.string.strip() if soup.title else \"\"\n",
    "\n",
    "        # Prefer <article> or <main> for main content\n",
    "        main_section = soup.find(\"article\") or soup.find(\"main\")\n",
    "        if main_section:\n",
    "            text = main_section.get_text(separator=\" \", strip=True)\n",
    "        else:\n",
    "            # Fallback: use all paragraphs and headings\n",
    "            tags = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\"])\n",
    "            text = \" \".join([t.get_text(\" \", strip=True) for t in tags])\n",
    "\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"body_text\": text,\n",
    "            \"word_count\": len(text.split())\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"title\": \"\", \"body_text\": \"\", \"word_count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679c8e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in dataset: 81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a7f46dc2414f578ef41039455f54b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing HTML:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted content saved to C:\\Users\\aishw\\seo-content-detector\\data\\extracted_content.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Cybersecurity is gaining more importance globa...</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Home Insights Blog Posts 11 Cyber Defense Tips...</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cybersecurity Best Practices CISA provides inf...</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                      \n",
       "\n",
       "                                           body_text  word_count  \n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         337  \n",
       "1  Cybersecurity is gaining more importance globa...        1700  \n",
       "2  Home Insights Blog Posts 11 Cyber Defense Tips...        1058  \n",
       "3  Cybersecurity Best Practices CISA provides inf...         826  \n",
       "4                                                              0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset & parse HTML\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(\"Rows in dataset:\", len(df))\n",
    "\n",
    "if 'html_content' not in df.columns:\n",
    "    raise ValueError(\"Expected 'html_content' column in data.csv.\")\n",
    "\n",
    "records = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Parsing HTML\"):\n",
    "    parsed = extract_from_html(row['html_content'])\n",
    "    records.append({\n",
    "        \"url\": row['url'],\n",
    "        \"title\": parsed['title'],\n",
    "        \"body_text\": parsed['body_text'],\n",
    "        \"word_count\": parsed['word_count']\n",
    "    })\n",
    "\n",
    "extracted_df = pd.DataFrame(records)\n",
    "extracted_df.to_csv(EXTRACTED_CSV, index=False)\n",
    "print(f\"‚úÖ Extracted content saved to {EXTRACTED_CSV}\")\n",
    "extracted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad027d",
   "metadata": {},
   "source": [
    "###  Text Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87318e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Using TF-IDF as embeddings.\n",
      "‚úÖ Features saved to C:\\Users\\aishw\\seo-content-detector\\data\\features.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>top_keywords</th>\n",
       "      <th>is_thin</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>337</td>\n",
       "      <td>6</td>\n",
       "      <td>-11.155568</td>\n",
       "      <td>cyber|cybersecurity|alliance|training|consultancy</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>1700</td>\n",
       "      <td>92</td>\n",
       "      <td>41.465000</td>\n",
       "      <td>varonis|data|access|security|app</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.017555493119617405, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>1058</td>\n",
       "      <td>62</td>\n",
       "      <td>53.262918</td>\n",
       "      <td>password|passphrase|don|authentication|cyber</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>826</td>\n",
       "      <td>27</td>\n",
       "      <td>-2.538002</td>\n",
       "      <td>cisa|cybersecurity|cyber|practices|resilience</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  word_count  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog         337   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips        1700   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...        1058   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...         826   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...           0   \n",
       "\n",
       "   sentence_count  flesch_reading_ease  \\\n",
       "0               6           -11.155568   \n",
       "1              92            41.465000   \n",
       "2              62            53.262918   \n",
       "3              27            -2.538002   \n",
       "4               0             0.000000   \n",
       "\n",
       "                                        top_keywords  is_thin  \\\n",
       "0  cyber|cybersecurity|alliance|training|consultancy     True   \n",
       "1                   varonis|data|access|security|app    False   \n",
       "2       password|passphrase|don|authentication|cyber    False   \n",
       "3      cisa|cybersecurity|cyber|practices|resilience    False   \n",
       "4                                                        True   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.017555493119617405, 0.0, 0.0, 0.0, 0.0...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Cleaning & Feature Engineering\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(sent_tokenize(text)) if text else 0\n",
    "\n",
    "extracted_df['clean_text'] = extracted_df['body_text'].apply(clean_text)\n",
    "extracted_df['sentence_count'] = extracted_df['clean_text'].apply(sentence_count)\n",
    "extracted_df['flesch_reading_ease'] = extracted_df['clean_text'].apply(lambda x: textstat.flesch_reading_ease(x) if x else 0)\n",
    "extracted_df['is_thin'] = extracted_df['word_count'] < 500\n",
    "\n",
    "# TF-IDF keywords\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(extracted_df['clean_text'])\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_keywords(row, n=5):\n",
    "    arr = row.toarray().flatten()\n",
    "    top_idx = arr.argsort()[-n:][::-1]\n",
    "    return \"|\".join([features[i] for i in top_idx if arr[i] > 0])\n",
    "\n",
    "top_keywords = [get_top_keywords(tfidf_matrix[i]) for i in range(tfidf_matrix.shape[0])]\n",
    "extracted_df['top_keywords'] = top_keywords\n",
    "\n",
    "# Embeddings\n",
    "if SENTE:\n",
    "    print(\"üîπ Using SentenceTransformer embeddings...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(extracted_df['clean_text'], show_progress_bar=True)\n",
    "else:\n",
    "    print(\"‚öôÔ∏è Using TF-IDF as embeddings.\")\n",
    "    embeddings = tfidf_matrix.toarray()\n",
    "\n",
    "extracted_df['embedding'] = [json.dumps(list(map(float, e))) for e in embeddings]\n",
    "\n",
    "# Save features\n",
    "features_df = extracted_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords', 'is_thin', 'embedding']]\n",
    "features_df.to_csv(FEATURES_CSV, index=False)\n",
    "print(f\"‚úÖ Features saved to {FEATURES_CSV}\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36319869",
   "metadata": {},
   "source": [
    "### Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19363b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Calculating cosine similarities...\n",
      "‚úÖ Duplicates saved to C:\\Users\\aishw\\seo-content-detector\\data\\duplicates.csv\n",
      "Total duplicate pairs found: 3\n",
      "Total pages: 81\n",
      "Thin content pages: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/SD-WAN</td>\n",
       "      <td>https://www.cisco.com/site/us/en/learn/topics/...</td>\n",
       "      <td>0.814697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://nytlicensing.com/latest/trends/content...</td>\n",
       "      <td>https://copyblogger.com/content-marketing/</td>\n",
       "      <td>0.801423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://nytlicensing.com/latest/trends/content...</td>\n",
       "      <td>https://www.coursera.org/articles/content-stra...</td>\n",
       "      <td>0.803941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url1  \\\n",
       "0               https://en.wikipedia.org/wiki/SD-WAN   \n",
       "1  https://nytlicensing.com/latest/trends/content...   \n",
       "2  https://nytlicensing.com/latest/trends/content...   \n",
       "\n",
       "                                                url2  similarity  \n",
       "0  https://www.cisco.com/site/us/en/learn/topics/...    0.814697  \n",
       "1         https://copyblogger.com/content-marketing/    0.801423  \n",
       "2  https://www.coursera.org/articles/content-stra...    0.803941  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Duplicate Detection\n",
    "print(\"üîç Calculating cosine similarities...\")\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "\n",
    "SIM_THRESHOLD = 0.8\n",
    "pairs = []\n",
    "\n",
    "for i in range(len(extracted_df)):\n",
    "    for j in range(i + 1, len(extracted_df)):\n",
    "        if cos_sim[i, j] >= SIM_THRESHOLD:\n",
    "            pairs.append({\n",
    "                \"url1\": extracted_df.loc[i, 'url'],\n",
    "                \"url2\": extracted_df.loc[j, 'url'],\n",
    "                \"similarity\": float(cos_sim[i, j])\n",
    "            })\n",
    "\n",
    "duplicates_df = pd.DataFrame(pairs)\n",
    "duplicates_df.to_csv(DUPLICATES_CSV, index=False)\n",
    "print(f\"‚úÖ Duplicates saved to {DUPLICATES_CSV}\")\n",
    "print(f\"Total duplicate pairs found: {len(duplicates_df)}\")\n",
    "\n",
    "print(\"Total pages:\", len(extracted_df))\n",
    "print(\"Thin content pages:\", extracted_df['is_thin'].sum())\n",
    "duplicates_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f57c",
   "metadata": {},
   "source": [
    "### Content Quality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91f09ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  High Quality       0.50      0.50      0.50         2\n",
      "   Low Quality       1.00      0.93      0.97        15\n",
      "Medium Quality       0.78      0.88      0.82         8\n",
      "\n",
      "      accuracy                           0.88        25\n",
      "     macro avg       0.76      0.77      0.76        25\n",
      "  weighted avg       0.89      0.88      0.88        25\n",
      "\n",
      "Overall Accuracy: 0.88\n",
      "F1 (weighted): 0.88\n",
      "Baseline Accuracy: 0.52\n",
      "\n",
      "Top Features:\n",
      "1. flesch_reading_ease (importance: 0.38)\n",
      "2. word_count (importance: 0.35)\n",
      "3. sentence_count (importance: 0.27)\n",
      "\n",
      "‚úÖ Model saved to C:\\Users\\aishw\\seo-content-detector\\models\\quality_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#  Quality Scoring Model\n",
    "\n",
    "# Step 1: Label assignment (synthetic rule-based)\n",
    "def quality_label(row):\n",
    "    wc, fr = row['word_count'], row['flesch_reading_ease']\n",
    "    if wc > 1500 and 50 <= fr <= 70:\n",
    "        return \"High Quality\"\n",
    "    elif wc < 500 or fr < 30:\n",
    "        return \"Low Quality\"\n",
    "    else:\n",
    "        return \"Medium Quality\"\n",
    "\n",
    "extracted_df['label'] = extracted_df.apply(quality_label, axis=1)\n",
    "\n",
    "# Step 2: Prepare features and labels\n",
    "X = extracted_df[['word_count', 'sentence_count', 'flesch_reading_ease']]\n",
    "y = extracted_df['label']\n",
    "\n",
    "# Step 3: Split dataset (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 4: Train model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate model\n",
    "print(\"Model Performance:\")\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "print(\"Overall Accuracy:\", round(accuracy_score(y_test, y_pred), 2))\n",
    "print(\"F1 (weighted):\", round(f1_score(y_test, y_pred, average='weighted'), 2))\n",
    "\n",
    "# Step 6: Baseline model (simple rule-based on word count)\n",
    "def baseline_rule(wc):\n",
    "    if wc > 1500:\n",
    "        return \"High Quality\"\n",
    "    elif wc < 500:\n",
    "        return \"Low Quality\"\n",
    "    else:\n",
    "        return \"Medium Quality\"\n",
    "\n",
    "baseline_pred = X_test['word_count'].apply(baseline_rule)\n",
    "baseline_acc = accuracy_score(y_test, baseline_pred)\n",
    "print(\"Baseline Accuracy:\", round(baseline_acc, 2))\n",
    "\n",
    "# Step 7: Feature importances\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop Features:\")\n",
    "for i, (feature, imp) in enumerate(importances.items(), start=1):\n",
    "    print(f\"{i}. {feature} (importance: {imp:.2f})\")\n",
    "\n",
    "# Step 8: Save model safely\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "print(f\"\\n‚úÖ Model saved to {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6c169",
   "metadata": {},
   "source": [
    "###  Real-Time Analysis Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5df70c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Main_Page\",\n",
      "  \"word_count\": 1971,\n",
      "  \"sentence_count\": 53,\n",
      "  \"readability\": 35.344451570758395,\n",
      "  \"is_thin\": false,\n",
      "  \"quality_label\": \"Medium Quality\",\n",
      "  \"similar_to\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#  Real-Time URL Analysis\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"seo-content-detector-bot/1.0\"}\n",
    "\n",
    "def scrape_html(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if res.status_code == 200:\n",
    "            return res.text\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def compute_embedding(text):\n",
    "    if SENTE:\n",
    "        return model.encode([text])[0]\n",
    "    return vectorizer.transform([text]).toarray()[0]\n",
    "\n",
    "def analyze_url(url):\n",
    "    html = scrape_html(url)\n",
    "    parsed = extract_from_html(html)\n",
    "    clean = clean_text(parsed['body_text'])\n",
    "    wc = parsed['word_count']\n",
    "    sc = sentence_count(clean)\n",
    "    fr = textstat.flesch_reading_ease(clean)\n",
    "    is_thin = wc < 500\n",
    "\n",
    "    emb = compute_embedding(clean)\n",
    "    sims = cosine_similarity([emb], embeddings)[0]\n",
    "    similar_pages = [\n",
    "        {\"url\": extracted_df.loc[i, 'url'], \"similarity\": float(sims[i])}\n",
    "        for i in np.where(sims > 0.75)[0]\n",
    "    ]\n",
    "\n",
    "    label = model.predict(pd.DataFrame([{\n",
    "        \"word_count\": wc, \"sentence_count\": sc, \"flesch_reading_ease\": fr\n",
    "    }]))[0]\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"word_count\": wc,\n",
    "        \"sentence_count\": sc,\n",
    "        \"readability\": fr,\n",
    "        \"is_thin\": is_thin,\n",
    "        \"quality_label\": label,\n",
    "        \"similar_to\": similar_pages\n",
    "    }\n",
    "\n",
    "result = analyze_url(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c113f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
